{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CS 3780/5780 Creative Project: </h2>\n",
    "<h3>Emotion Classification of Natural Language</h3>\n",
    "\n",
    "Names and NetIDs for your group members:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction:</h3>\n",
    "\n",
    "<p> The creative project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The past programming projects provide templates for how to do this (and you can reuse part of your code if you wish), and the lectures provide some of the methods you can use. So, this creative project brings realism to how you will use machine learning in the real world.  </p>\n",
    "\n",
    "The task you will work on is classifying texts to human emotions. Through words, humans express feelings, articulate thoughts, and communicate our deepest needs and desires. Language helps us interpret the nuances of joy, sadness, anger, and love, allowing us to connect with others on a deeper level. Are you able to train an ML model that recognizes the human emotions expressed in a piece of text? <b>Please read the project description PDF file carefully and follow the instructions there. Also make sure you write your code and answers to all the questions in this Jupyter Notebook </b> </p>\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 0: Preliminaries</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.1 Import:</h3><p>\n",
    "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "    \n",
    "https://pytorch.org/tutorials/\n",
    "    \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# TODO\n",
    "\n",
    "# student-imported libraries are listed below\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for preprocessing\n",
    "import re # imports the regular expression module, which provides support for working w/ text\n",
    "from sklearn.model_selection import train_test_split # allows us to split our bag of words and labels for training\n",
    "\n",
    "# algorithm 1: SVM\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "# algorithm 2: MLP\n",
    "# new torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# new vision-dataset-related torch imports\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.2 Accuracy:</h3><p>\n",
    "To measure your performance in the Kaggle Competition, we are using accuracy. As a recap, accuracy is the percent of labels you predict correctly. To measure this, you can use library functions from sklearn. A simple example is shown below. \n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = [3, 2, 1, 0, 1, 2, 3]\n",
    "y_true = [0, 1, 2, 3, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: Basics</h2><p>\n",
    "Note that your code should be commented well and in part 1.4 you can refer to your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1 Load and preprocess the dataset:</h3><p>\n",
    "We provide how to load the data on Kaggle's Notebook.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/cs-3780-5780-how-do-you-feel/train.csv\")\n",
    "train_text = train[\"text\"]\n",
    "train_label = train[\"label\"]\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/cs-3780-5780-how-do-you-feel/test.csv\")\n",
    "test_id = test[\"id\"]\n",
    "test_text = test[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading the data for testing on local machine. delete this code segment or comment out when submitting\n",
    "#train = pd.read_csv(\"train.csv\")\n",
    "#train_text = train[\"text\"]\n",
    "#train_label = train[\"label\"]\n",
    "\n",
    "\n",
    "#test = pd.read_csv(\"test.csv\")\n",
    "#test_id = test[\"id\"]\n",
    "#test_text = test[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([8000, 12000])\n",
      "y_train_tensor shape: torch.Size([8000, 1])\n"
     ]
    }
   ],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 1.4\n",
    "# train \n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    takes a given string, and normalizes it by doing the following\n",
    "    1. lowercase all text to keep consistency\n",
    "    2. remove any punctuation and numbers\n",
    "    \n",
    "    returns the cleaned text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# preprocess the text \n",
    "train_text_cleaned = train_text.apply(preprocess_text)\n",
    "# print(df)\n",
    "\n",
    "# converting the processed text into a bag-of-words vector\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=12000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(train_text_cleaned)\n",
    "y = train_label\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train.toarray())\n",
    "X_test_tensor = torch.FloatTensor(X_test.toarray())\n",
    "\n",
    "X_train_tensor = X_train_tensor.float()\n",
    "X_test_tensor = X_test_tensor.float()\n",
    "\n",
    "# Convert y_train and y_test from Pandas Series to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).to(X_train_tensor.device)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).to(X_test_tensor.device)\n",
    "\n",
    "# Ensure the shapes are compatible\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)  # Make it (n, 1) for regression\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "\n",
    "#print(f\"X_train_tensor shape: {X_train_tensor.shape}\")\n",
    "#print(f\"y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "#print(X_train_tensor.shape[1])\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 12000)\n"
     ]
    }
   ],
   "source": [
    "### code to preprocess the test data set...\n",
    "test_text_cleaned = test_text.apply(preprocess_text)\n",
    "# print(df)\n",
    "\n",
    "# converting the processed text into a bag-of-words vector\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "#vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the text data\n",
    "test_x = vectorizer.transform(test_text_cleaned)\n",
    "test_x_tensor = torch.FloatTensor(test_x.toarray())\n",
    "test_x_tensor = test_x_tensor.float()\n",
    "print(test_x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
    "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nonlinear_data(num_samples=10000):\n",
    "    # generate random x samples for training and test sets\n",
    "    #note: this function is not used\n",
    "    xTr = torch.rand(num_samples, 1) * 2 * np.pi\n",
    "    xTe = torch.rand(int(num_samples * 0.1), 1) * 2 * np.pi\n",
    "    \n",
    "    # gaussian noise for non-linear regression\n",
    "    noise = torch.rand(num_samples, 1) * 0.2\n",
    "    test_noise = torch.rand(int(num_samples * 0.1), 1) * 0.2\n",
    "    \n",
    "    # add noise on the labels for the training set\n",
    "    yTr = torch.sin(xTr) + noise\n",
    "    yTe = torch.sin(xTe) + test_noise\n",
    "    \n",
    "    #print(xTr.shape)\n",
    "    #print(\"PING\")\n",
    "    return xTr, xTe, yTr, yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 2.2906038761138916\n",
      "epoch 200 loss 2.1565213203430176\n",
      "epoch 300 loss 2.046164035797119\n",
      "epoch 400 loss 1.956129550933838\n",
      "epoch 500 loss 1.8857115507125854\n",
      "epoch 600 loss 1.8211930990219116\n",
      "epoch 700 loss 1.7543877363204956\n",
      "epoch 800 loss 1.6822150945663452\n",
      "epoch 900 loss 1.6046010255813599\n",
      "epoch 1000 loss 1.5233269929885864\n",
      "epoch 1100 loss 1.4410161972045898\n",
      "epoch 1200 loss 1.3603034019470215\n",
      "epoch 1300 loss 1.283331036567688\n",
      "epoch 1400 loss 1.211527705192566\n",
      "epoch 1500 loss 1.1456547975540161\n",
      "epoch 1600 loss 1.0859127044677734\n",
      "epoch 1700 loss 1.0321239233016968\n",
      "epoch 1800 loss 0.983841061592102\n",
      "epoch 1900 loss 0.9405023455619812\n",
      "epoch 2000 loss 0.9015121459960938\n",
      "epoch 2100 loss 0.8663039803504944\n",
      "epoch 2200 loss 0.8343588709831238\n",
      "epoch 2300 loss 0.805232048034668\n",
      "epoch 2400 loss 0.7785080671310425\n",
      "epoch 2500 loss 0.75382000207901\n",
      "epoch 2600 loss 0.7308610677719116\n",
      "epoch 2700 loss 0.709376871585846\n",
      "epoch 2800 loss 0.6891511082649231\n",
      "epoch 2900 loss 0.6700173020362854\n",
      "epoch 3000 loss 0.6518246531486511\n",
      "epoch 3100 loss 0.6344524025917053\n",
      "epoch 3200 loss 0.6178151965141296\n",
      "epoch 3300 loss 0.6018437147140503\n",
      "epoch 3400 loss 0.5864781141281128\n",
      "epoch 3500 loss 0.5716620087623596\n",
      "epoch 3600 loss 0.5573570132255554\n",
      "epoch 3700 loss 0.5435205698013306\n",
      "epoch 3800 loss 0.5301247239112854\n",
      "epoch 3900 loss 0.517140805721283\n",
      "epoch 4000 loss 0.5045469403266907\n",
      "epoch 4100 loss 0.49231672286987305\n",
      "epoch 4200 loss 0.48043176531791687\n",
      "epoch 4300 loss 0.4688766300678253\n",
      "epoch 4400 loss 0.45763471722602844\n",
      "epoch 4500 loss 0.4466920495033264\n",
      "epoch 4600 loss 0.4360325336456299\n",
      "epoch 4700 loss 0.42564380168914795\n",
      "epoch 4800 loss 0.41551604866981506\n",
      "epoch 4900 loss 0.40563851594924927\n",
      "epoch 5000 loss 0.3959973156452179\n",
      "epoch 5100 loss 0.3865853250026703\n",
      "epoch 5200 loss 0.37739676237106323\n",
      "epoch 5300 loss 0.3684258759021759\n",
      "epoch 5400 loss 0.3596639335155487\n",
      "epoch 5500 loss 0.35110634565353394\n",
      "epoch 5600 loss 0.34274935722351074\n",
      "epoch 5700 loss 0.3345874547958374\n",
      "epoch 5800 loss 0.32661688327789307\n",
      "epoch 5900 loss 0.3188319802284241\n",
      "epoch 6000 loss 0.31122899055480957\n",
      "epoch 6100 loss 0.30380094051361084\n",
      "epoch 6200 loss 0.29654592275619507\n",
      "epoch 6300 loss 0.2894613444805145\n",
      "epoch 6400 loss 0.2825438976287842\n",
      "epoch 6500 loss 0.2757912576198578\n",
      "epoch 6600 loss 0.2691997289657593\n",
      "epoch 6700 loss 0.2627650797367096\n",
      "epoch 6800 loss 0.25648626685142517\n",
      "epoch 6900 loss 0.2503601014614105\n",
      "epoch 7000 loss 0.24438302218914032\n",
      "epoch 7100 loss 0.23855090141296387\n",
      "epoch 7200 loss 0.2328614443540573\n",
      "epoch 7300 loss 0.22731192409992218\n",
      "epoch 7400 loss 0.22189924120903015\n",
      "epoch 7500 loss 0.21662084758281708\n",
      "epoch 7600 loss 0.21147504448890686\n",
      "epoch 7700 loss 0.20645906031131744\n",
      "epoch 7800 loss 0.201568603515625\n",
      "epoch 7900 loss 0.19680173695087433\n",
      "epoch 8000 loss 0.19215676188468933\n",
      "epoch 8100 loss 0.18763121962547302\n",
      "epoch 8200 loss 0.18322262167930603\n",
      "epoch 8300 loss 0.17892925441265106\n",
      "epoch 8400 loss 0.17474879324436188\n",
      "epoch 8500 loss 0.17067855596542358\n",
      "epoch 8600 loss 0.1667165905237198\n",
      "epoch 8700 loss 0.16286012530326843\n",
      "epoch 8800 loss 0.15910692512989044\n",
      "epoch 8900 loss 0.15545496344566345\n",
      "epoch 9000 loss 0.15190193057060242\n",
      "epoch 9100 loss 0.1484459489583969\n",
      "epoch 9200 loss 0.14508390426635742\n",
      "epoch 9300 loss 0.1418144255876541\n",
      "epoch 9400 loss 0.13863486051559448\n",
      "epoch 9500 loss 0.13554391264915466\n",
      "epoch 9600 loss 0.13253910839557648\n",
      "epoch 9700 loss 0.1296180635690689\n",
      "epoch 9800 loss 0.12677866220474243\n",
      "epoch 9900 loss 0.1240190714597702\n",
      "epoch 10000 loss 0.1213378831744194\n",
      "Done\n",
      "tensor([27, 16, 21,  ..., 12,  1,  4])\n",
      "Program started at: Thu Dec  5 16:22:03 2024\n",
      "Program ended at: Thu Dec  5 16:54:35 2024\n",
      "Total elapsed time: 1951.48105 seconds\n"
     ]
    }
   ],
   "source": [
    "### MLP\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    square_diff = torch.pow((y_pred-y_true), 2)\n",
    "    mean_error = 0.5 * torch.mean(square_diff)\n",
    "    return mean_error\n",
    "\n",
    "class CustomSGD(optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        defaults = dict(lr=lr, momentum=momentum)\n",
    "        super(CustomSGD, self).__init__(params, defaults)\n",
    "        self.velocities = [torch.zeros_like(param.data) for param in self.param_groups[0]['params']]\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update the parameters with velocity and gradient.\n",
    "        There is nothing needed to return from the function.\n",
    "        Please update the param.data directly.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for param, velocity in zip(group['params'], self.velocities):\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                lr = group['lr'] # learning rate\n",
    "                momentum = group['momentum'] # momentum coefficient\n",
    "                gradient = param.grad.data # gradient\n",
    "                \n",
    "                # update the velocity; [:] enables inplace update\n",
    "                # velocity[:] = None\n",
    "                \n",
    "                # update the parameters\n",
    "                # param.data = None\n",
    "                velocity[:] = momentum * velocity[:] + (1-momentum) * gradient\n",
    "                param.data = param.data - lr * velocity[:]\n",
    "                \n",
    "                \n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=28):\n",
    "        super(MLPNet, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initialize the fully connected layers\n",
    "        # raise NotImplementedError(\"Your code goes here!\")\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass, with ReLU non-linearities\n",
    "        # raise NotImplementedError(\"Your code goes here!\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        #return F.softmax(x, dim=1) # added\n",
    "\n",
    "def train_regression_model(xTr, yTr, model, num_epochs, lr=1e-2, momentum=0.9, print_freq=100, display_loss=True):\n",
    "    \"\"\"Train loop for a neural network model.\n",
    "    \n",
    "    Input:\n",
    "        xTr:     (n, d) matrix of regression input data\n",
    "        yTr:     n-dimensional vector of regression labels\n",
    "        model:   nn.Model to be trained\n",
    "        num_epochs: number of epochs to train the model for\n",
    "        lr:      learning rate for the optimizer\n",
    "        print_freq: frequency to display the loss\n",
    "        display_loss: boolean, if we print the loss\n",
    "    \n",
    "    Output:\n",
    "        model:   nn.Module trained model\n",
    "    \"\"\"\n",
    "    optimizer = CustomSGD(model.parameters(), lr=lr, momentum=momentum)  # create an Adam optimizer for the model parameters\n",
    "    \n",
    "     # Should be (batch_size,)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        # need to zero the gradients in the optimizer so we don't\n",
    "        # use the gradients from previous iterations\n",
    "        optimizer.zero_grad()  \n",
    "        pred = model(xTr)  # run the forward pass through the model to compute predictions\n",
    "        #print(\"Prediction shape:\", pred.shape)  # Should be (batch_size, num_classes)\n",
    "        #print(f\"Prediction shape: {pred.shape}\")  # Should be (batch_size, num_classes)\n",
    "        #print(f\"Target shape: {yTr.shape}\") \n",
    "        #loss = mse_loss(pred, yTr)\n",
    "        yTr = yTr.view(-1).long()\n",
    "        loss = criterion(pred, yTr)\n",
    "        loss.backward()  # compute the gradient wrt loss\n",
    "        optimizer.step()  # performs a step of gradient descent\n",
    "        if display_loss and (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return model  # return trained model\n",
    "\n",
    "\n",
    "hdims = 69\n",
    "num_epochs = 10000\n",
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = gen_nonlinear_data(num_samples=500)\n",
    "\n",
    "size = X_train_tensor.shape[1]\n",
    "mlp_model = MLPNet(input_dim=size, hidden_dim=hdims, output_dim=28)\n",
    "\n",
    "mlp_model = train_regression_model(X_train_tensor, y_train_tensor, mlp_model, num_epochs=num_epochs, lr=lr, momentum=momentum)\n",
    "#mlp_model = train_regression_model(X_train, y_train, mlp_model, num_epochs=num_epochs, lr=lr, momentum=momentum)\n",
    "\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = mlp_model(test_x_tensor)\n",
    "\n",
    "# Convert raw outputs to class predictions (e.g., using argmax for multi-class)\n",
    "y_test_pred_classes = torch.argmax(y_test_pred, dim=1)\n",
    "\n",
    "print(\"Done\")\n",
    "print(y_test_pred_classes)\n",
    "\n",
    "\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "#X_train_tensor_1D = X_train_tensor[:, 0]  # Use the first feature\n",
    "#X_test_tensor_1D = X_test_tensor[:, 0]\n",
    "\n",
    "# Plot the visualizations from our MLP Model\n",
    "#ax1.scatter(X_train_tensor, y_train_tensor, label=\"Train Points\")\n",
    "#ax1.scatter(X_test_tensor, y_test_tensor, label=\"Test Points\")\n",
    "#ax1.scatter(X_train_tensor, mlp_model(X_train_tensor).detach(), color=\"red\", marker='o', label=\"Prediction\")\n",
    "\n",
    "#ax1.scatter(X_train, y_train, label=\"Train Points\")\n",
    "#ax1.scatter(X_test, y_test, label=\"Test Points\")\n",
    "#ax1.scatter(X_train, mlp_model(X_train).detach(), color=\"red\", marker='o', label=\"Prediction\")\n",
    "\n",
    "#ax1.scatter(X_train_tensor_1D.cpu().numpy(), y_train_tensor.squeeze().cpu().numpy(), label=\"Train Points\")\n",
    "#ax1.scatter(X_test_tensor_1D.cpu().numpy(), y_test_tensor.squeeze().cpu().numpy(), label=\"Test Points\")\n",
    "#ax1.scatter(X_train_tensor_1D.cpu().numpy(), mlp_model(X_train_tensor).detach().squeeze().cpu().numpy(), color=\"red\", marker='o', label=\"Prediction\")\n",
    "\n",
    "#ax1.legend()\n",
    "#ax1.set_title('MLP Net')\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Program started at: {time.ctime(start_time)}\")\n",
    "print(f\"Program ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Total elapsed time: {elapsed_time:.5f} seconds\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 Training, Validation and Model Selection:</h3><p>\n",
    "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "21    0.262800\n",
      "1     0.218533\n",
      "12    0.137867\n",
      "9     0.094867\n",
      "4     0.074667\n",
      "27    0.064267\n",
      "16    0.026400\n",
      "17    0.024333\n",
      "25    0.017200\n",
      "10    0.014400\n",
      "8     0.012267\n",
      "22    0.010200\n",
      "18    0.008067\n",
      "24    0.007267\n",
      "11    0.005867\n",
      "23    0.004800\n",
      "20    0.003867\n",
      "15    0.003533\n",
      "6     0.003467\n",
      "3     0.003000\n",
      "19    0.002000\n",
      "5     0.000133\n",
      "14    0.000067\n",
      "7     0.000067\n",
      "2     0.000067\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 1.4\n",
    "y_preds = y_test_pred_classes.numpy()\n",
    "print(\"Done\")\n",
    "print()\n",
    "value_frequency = pd.value_counts(y_preds, sort=True, normalize = True)\n",
    "print(value_frequency)\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4 Explanation in Words:</h3><p>\n",
    "    You need to answer the following questions in the markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.1 How did you formulate the learning problem?\n",
    "\n",
    "1.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "1.4.3 How did you do the model selection?\n",
    "\n",
    "1.4.4 Does the test performance reach the first baseline \"Tiny Piney\"? (Please include a screenshot of Kaggle Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Be creative!</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Open-ended Code:</h3><p>\n",
    "You may follow the steps in part 1 again but making innovative changes like using new training algorithms, etc. Make sure you explain everything clearly in part 2.2. Note that beating \"Zero Hero\" is only a portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 2.2\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 Explanation in Words:</h3><p>\n",
    "You need to answer the following questions in a markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 How much did you manage to improve performance on the test set? Did you beat \"Zero Hero\" in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "2.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Kaggle Submission</h2><p>\n",
    "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The results should be presented in two columns in csv format: the first column is the data id (0-14999) and the second column includes the predictions for the test set. The first column must be named id and the second column must be named label (otherwise your submission will fail). A sample predication file can be downloaded from Kaggle for each problem. \n",
    "We provide how to save a csv file if you are running Notebook on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = range(15000)\n",
    "prediction = range(15000)\n",
    "submission = pd.DataFrame({'id': id, 'label': prediction})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# You may use pandas to generate a dataframe with country, date and your predictions first \n",
    "# and then use to_csv to generate a CSV file.\n",
    "\n",
    "\n",
    "id = test_id\n",
    "prediction = y_preds\n",
    "submission = pd.DataFrame({'id': id, 'label': prediction})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 4: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please cite the papers and open resources you used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
